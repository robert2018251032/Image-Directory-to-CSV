{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Plant-diseases-classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPShWG3YbzSGpYd/hzZ7Squ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robert2018251032/Image-Directory-to-CSV/blob/master/Plant_diseases_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGxzi9VXx8bq"
      },
      "source": [
        "Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkZGEjzixa1x"
      },
      "source": [
        "#https://github.com/viritaromero/Plant-diseases-classifier/blob/master/Plant_diseased_classifier.ipynb\n",
        "# We need pillow version of 5.3.0\n",
        "# We will uninstall the older version first\n",
        "!pip uninstall -y Pillow\n",
        "# Install the new one\n",
        "!pip install Pillow==5.3.0\n",
        "# Let's verify the version\n",
        "# This should print 5.3.0. If it doesn't, then restart your runtime:\n",
        "# Menu > Runtime > Restart Runtime\n",
        "!pip install image\n",
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "import PIL\n",
        "print(PIL.PILLOW_VERSION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYC9ECOoxygc"
      },
      "source": [
        "\n",
        "# We will verify that GPU is enabled for this notebook\n",
        "# Following should print: CUDA is available!  Training on GPU ...\n",
        "# if it prints otherwise, then you need to enable GPU: \n",
        "# From Menu > Runtime > Change Runtime Type > Hardware Accelerator > GPU\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import torchvision\n",
        "from collections import OrderedDict\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "from os.path import exists\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmHywet0yUU9"
      },
      "source": [
        "Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49y5vscHxyqO"
      },
      "source": [
        "# Download the dataset and unzip de folder\n",
        "!gdown https://drive.google.com/uc?id=1Bhh3VeMBH6F7vKqHdDDmdyi-7RfvQNqJ\n",
        "!tar -xvf PlantVillage.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvYXa5XPxysN"
      },
      "source": [
        "#Organizing the dataset\n",
        "data_dir = '/PlantVillage'\n",
        "train_dir = data_dir + '/train'\n",
        "valid_dir = data_dir + '/val'\n",
        "nThreads = 4\n",
        "batch_size = 32\n",
        "use_gpu = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz9HJqUFydRW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX2blk56ydpE"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('categories.json', 'r') as f:\n",
        "    cat_to_name = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTakstn2yduF"
      },
      "source": [
        "# Define your transforms for the training and validation sets\n",
        "# Data augmentation and normalization for training\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        #transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Load the datasets with ImageFolder\n",
        "\n",
        "data_dir = 'PlantVillage'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "\n",
        "# Using the image datasets and the trainforms, define the dataloaders\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "class_names = image_datasets['train'].classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpzwYvNlynRl"
      },
      "source": [
        "Building and training the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtJJJ4HKydyX"
      },
      "source": [
        "# Build and train your network\n",
        "\n",
        "# 1. Load resnet-152 pre-trained network\n",
        "model = models.resnet152(pretrained=True)\n",
        "# Freeze parameters so we don't backprop through them\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#Let's check the model architecture:\n",
        "    print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrCpcKWlydzy"
      },
      "source": [
        "# 2. Define a new, untrained feed-forward network as a classifier, using ReLU activations\n",
        "\n",
        "# Our input_size matches the in_features of pretrained model\n",
        "\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "# Creating the classifier ordered dictionary first\n",
        "\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(2048, 512)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('fc2', nn.Linear(512, 39)),\n",
        "                          ('output', nn.LogSoftmax(dim=1))\n",
        "                          ]))\n",
        "\n",
        "# Replacing the pretrained model classifier with our classifier\n",
        "model.fc = classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--0gSW1syd1f"
      },
      "source": [
        "#Function to train the model\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=20):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best valid accuracy: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIoblqOSyd4O"
      },
      "source": [
        "# Train a model with a pre-trained network\n",
        "num_epochs = 10\n",
        "if use_gpu:\n",
        "    print (\"Using GPU: \"+ str(use_gpu))\n",
        "    model = model.cuda()\n",
        "\n",
        "# NLLLoss because our output is LogSoftmax\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Adam optimizer with a learning rate\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "# Decay LR by a factor of 0.1 every 5 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "\n",
        "model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB_xS0ZAy3oB"
      },
      "source": [
        "# Do validation on the test set\n",
        "def test(model, dataloaders, device):\n",
        "  model.eval()\n",
        "  accuracy = 0\n",
        "  \n",
        "  model.to(device)\n",
        "    \n",
        "  for images, labels in dataloaders['val']:\n",
        "    images = Variable(images)\n",
        "    labels = Variable(labels)\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "      \n",
        "    output = model.forward(images)\n",
        "    ps = torch.exp(output)\n",
        "    equality = (labels.data == ps.max(1)[1])\n",
        "    accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
        "      \n",
        "    print(\"Testing Accuracy: {:.3f}\".format(accuracy/len(dataloaders['val'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2elHpauy3sS"
      },
      "source": [
        "test(model, dataloaders, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA4JMlRbzAq0"
      },
      "source": [
        "\n",
        "Save the checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6uUctCLzBiG"
      },
      "source": [
        "# Save the checkpoint \n",
        "\n",
        "model.class_to_idx = dataloaders['train'].dataset.class_to_idx\n",
        "model.epochs = num_epochs\n",
        "checkpoint = {'input_size': [3, 224, 224],\n",
        "                 'batch_size': dataloaders['train'].batch_size,\n",
        "                  'output_size': 39,\n",
        "                  'state_dict': model.state_dict(),\n",
        "                  'data_transforms': data_transforms,\n",
        "                  'optimizer_dict':optimizer.state_dict(),\n",
        "                  'class_to_idx': model.class_to_idx,\n",
        "                  'epoch': model.epochs}\n",
        "torch.save(checkpoint, 'plants9615_checkpoint.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN-KR92ozHH6"
      },
      "source": [
        "Loading the checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTQPSBjzzBlW"
      },
      "source": [
        "#Download the trained model from here: \n",
        "!gdown https://drive.google.com/uc?id=1D3mWC5AAWlx3OdU4yljt7vtEPrpLFtmZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2Zgp91wxyt4"
      },
      "source": [
        "# Write a function that loads a checkpoint and rebuilds the model\n",
        "\n",
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = models.resnet152()\n",
        "    \n",
        "    # Our input_size matches the in_features of pretrained model\n",
        "    input_size = 2048\n",
        "    output_size = 39\n",
        "    \n",
        "    classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(2048, 512)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          #('dropout1', nn.Dropout(p=0.2)),\n",
        "                          ('fc2', nn.Linear(512, 39)),\n",
        "                          ('output', nn.LogSoftmax(dim=1))\n",
        "                          ]))\n",
        "\n",
        "# Replacing the pretrained model classifier with our classifier\n",
        "    model.fc = classifier\n",
        "    \n",
        "    \n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    \n",
        "    return model, checkpoint['class_to_idx']\n",
        "\n",
        "# Get index to class mapping\n",
        "loaded_model, class_to_idx = load_checkpoint('plants9615_checkpoint.pth')\n",
        "idx_to_class = { v : k for k,v in class_to_idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfAdG2Yezc6t"
      },
      "source": [
        "Inference for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4yQqeafzcBR"
      },
      "source": [
        "def process_image(image):\n",
        "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
        "        returns an Numpy array\n",
        "    '''\n",
        "    \n",
        "    # Process a PIL image for use in a PyTorch model\n",
        "\n",
        "    size = 256, 256\n",
        "    image.thumbnail(size, Image.ANTIALIAS)\n",
        "    image = image.crop((128 - 112, 128 - 112, 128 + 112, 128 + 112))\n",
        "    npImage = np.array(image)\n",
        "    npImage = npImage/255.\n",
        "        \n",
        "    imgA = npImage[:,:,0]\n",
        "    imgB = npImage[:,:,1]\n",
        "    imgC = npImage[:,:,2]\n",
        "    \n",
        "    imgA = (imgA - 0.485)/(0.229) \n",
        "    imgB = (imgB - 0.456)/(0.224)\n",
        "    imgC = (imgC - 0.406)/(0.225)\n",
        "        \n",
        "    npImage[:,:,0] = imgA\n",
        "    npImage[:,:,1] = imgB\n",
        "    npImage[:,:,2] = imgC\n",
        "    \n",
        "    npImage = np.transpose(npImage, (2,0,1))\n",
        "    \n",
        "    return npImage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6xVUDsazcE3"
      },
      "source": [
        "def imshow(image, ax=None, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    \n",
        "    # PyTorch tensors assume the color channel is the first dimension\n",
        "    # but matplotlib assumes is the third dimension\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    # Undo preprocessing\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    \n",
        "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
        "    image = np.clip(image, 0, 1)\n",
        "    \n",
        "    ax.imshow(image)\n",
        "    \n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN0KJ4e8zoLy"
      },
      "source": [
        "Class Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NVsaQoZzcJV"
      },
      "source": [
        "def predict(image_path, model, topk=5):\n",
        "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
        "    '''\n",
        "    \n",
        "    # Implement the code to predict the class from an image file\n",
        "    \n",
        "    image = torch.FloatTensor([process_image(Image.open(image_path))])\n",
        "    model.eval()\n",
        "    output = model.forward(Variable(image))\n",
        "    pobabilities = torch.exp(output).data.numpy()[0]\n",
        "    \n",
        "\n",
        "    top_idx = np.argsort(pobabilities)[-topk:][::-1] \n",
        "    top_class = [idx_to_class[x] for x in top_idx]\n",
        "    top_probability = pobabilities[top_idx]\n",
        "\n",
        "    return top_probability, top_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UepwBJ1ZzcN3"
      },
      "source": [
        "print (predict('PlantVillage/val/Blueberry___healthy/06eacfab-fb39-40e0-bbce-927bc98fa2ac___RS_HL 2663.JPG', loaded_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7urs681z1Ub"
      },
      "source": [
        "Sanity Checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts6kzLb7xyvR"
      },
      "source": [
        "# Display an image along with the top 5 classes\n",
        "def view_classify(img, probabilities, classes, mapper):\n",
        "    ''' Function for viewing an image and it's predicted classes.\n",
        "    '''\n",
        "    img_filename = img.split('/')[-2]\n",
        "    img = Image.open(img)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,10), ncols=1, nrows=2)\n",
        "    flower_name = mapper[img_filename]\n",
        "    \n",
        "    ax1.set_title(flower_name)\n",
        "    ax1.imshow(img)\n",
        "    ax1.axis('off')\n",
        "    \n",
        "    y_pos = np.arange(len(probabilities))\n",
        "    ax2.barh(y_pos, probabilities)\n",
        "    ax2.set_yticks(y_pos)\n",
        "    ax2.set_yticklabels([mapper[x] for x in classes])\n",
        "    ax2.invert_yaxis()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8lcEhptz7en"
      },
      "source": [
        "#img = 'PlantVillage/val/Apple___Black_rot/0139bc6d-391c-4fd1-bcae-cc74dabfddd7___JR_FrgE.S 2734.JPG'\n",
        "#img = 'PlantVillage/val/Tomato___Bacterial_spot/00728f4d-83a0-49f1-87f8-374646fcda05___GCREC_Bact.Sp 6326.JPG'\n",
        "img = 'PlantVillage/val/Corn_(maize)___Northern_Leaf_Blight/00a14441-7a62-4034-bc40-b196aeab2785___RS_NLB 3932.JPG'\n",
        "#img = 'PlantVillage/val/Apple___healthy/3af9dc00-a64b-4b45-a034-1d190e5277ea___RS_HL 7788.JPG'\n",
        "#img = 'PlantVillage/val/Potato___Late_blight/0acdc2b2-0dde-4073-8542-6fca275ab974___RS_LB 4857.JPG'\n",
        "#img = 'PlantVillage/val/Tomato___Tomato_Yellow_Leaf_Curl_Virus/0e1fda76-d958-490f-9fcb-21e86c99dbe6___UF.GRC_YLCV_Lab 02200.JPG'\n",
        "\n",
        "p, c = predict(img, loaded_model)\n",
        "view_classify(img, p, c, cat_to_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDNQ46kmz7hr"
      },
      "source": [
        "\n",
        "img = 'PlantVillage/val/Tomato___Tomato_Yellow_Leaf_Curl_Virus/0e1fda76-d958-490f-9fcb-21e86c99dbe6___UF.GRC_YLCV_Lab 02200.JPG'\n",
        "p, c = predict(img, loaded_model)\n",
        "view_classify(img, p, c, cat_to_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQu_gM-iz_ij"
      },
      "source": [
        "\n",
        "img = 'PlantVillage/val/Apple___Black_rot/0139bc6d-391c-4fd1-bcae-cc74dabfddd7___JR_FrgE.S 2734.JPG'\n",
        "p, c = predict(img, loaded_model)\n",
        "view_classify(img, p, c, cat_to_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EiKVEd3z_lr"
      },
      "source": [
        "\n",
        "img = 'PlantVillage/val/Tomato___Bacterial_spot/00728f4d-83a0-49f1-87f8-374646fcda05___GCREC_Bact.Sp 6326.JPG'\n",
        "p, c = predict(img, loaded_model)\n",
        "view_classify(img, p, c, cat_to_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8CUrl8b0Edt"
      },
      "source": [
        "img = 'PlantVillage/val/Apple___healthy/3af9dc00-a64b-4b45-a034-1d190e5277ea___RS_HL 7788.JPG'\n",
        "p, c = predict(img, loaded_model)\n",
        "view_classify(img, p, c, cat_to_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLlgf_FR0Egz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2ZKyMuk0ElS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PmSRCfRx7pD"
      },
      "source": [
        ""
      ]
    }
  ]
}